{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability theory \n",
    "\n",
    "The world we live in is inherently random. Even in a fully deterministic universe - quantum mechanics seem to contradict this philosophy, at least for now - we would require complete knowledge of all relevant variables and factors in order to perform inference and prediction on said problem. Given incomplete knowledge about the data generating process, either in the form of missing variables or due to the mere fact that we can only draw samples from that process, we have to live with some form of randomness. Probability theory now becomes an important tool to solve data related problems.  \n",
    "\n",
    "<br>\n",
    "\n",
    "Introductory courses on probability theory are mostly concerned with well-behaved random variables like Bernoulli and Binomials and the famous Normal distribution. While this is certaintly helpful to learn the basics or derive meaningful theoretical results, we need to be aware that real-world data problems are often messy and our typical assumptions might break down completely. Outside of academia, empirical results outweigh theoretical justification and even the most elegant model will quickly be dropped if it makes you lose money. [This story](https://towardsdatascience.com/5-things-academics-need-to-know-when-they-become-data-scientists-591d078e6ef6) is a somewhat cautionary tale about this issue.  \n",
    "\n",
    "<br>\n",
    "\n",
    "Nevertheless, theory can guide our way through the sheer endless possible solutions for a given problem and help us exclude those options that clearly won't work. Since every problem is different, a data practitioner with strong mathematical and statistical knowledge could also develop new solutions that work exceptionally well for that particular task at hand. As often in life, balance is key here.  \n",
    "\n",
    "<br>\n",
    "\n",
    "With that being said, I hope that this section can provide helpful insights on the theory of probability. I certainly cannot cover everything but aim to give a reasonable amount of sources for further reading. There will also be proofs, as a solid proof would still outweigh every other argument when searching for the most performant approach. If you find that the sample average will be a better solution than a 10,000\\$ Deep Bayesian Residual Neural Network trained with a hybrid MCMC algorithm, you better be able to prove that to your hype-driven manager. A savvy sales team might still be able to somehow sell that as advanced Artificial Intelligence.  \n",
    "\n",
    "<br>  \n",
    "\n",
    "Also a final quick note: All content will for now be based on the typical notion of probability without measure-theoretic background as is typically done for non-mathematicians. While measure theory generalizes many aspects of probability theory and advanced Machine Learning research frequently taps into it, I found it too complex to discuss it here as well (I also don't feel confident yet to provide learning material on it). For a measure theoretic introduction into probability, I found [this book by David Pollard](https://books.google.de/books/about/A_User_s_Guide_to_Measure_Theoretic_Prob.html?id=B7Ch-c2G21MC&redir_esc=y) to be a very good read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important inequalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hoeffding's inequality  \n",
    "\n",
    "### 1) Hoeffding's lemma  \n",
    "**Proposition:** For $X$ a random variable with $\\mathbb{E}\\left[X\\right]=0$ and $a\\leq X\\leq b,\\,b>a$ and any $t>0$:  \n",
    "\n",
    "$$\\mathbb{E}\\left[e^{tX}\\right]\\leq e^\\frac{t^2(b-a)^2}{8}$$  \n",
    "**Proof:** By convexity of $e^x$, we have  \n",
    "\n",
    "$$e^{tx}\\leq\\frac{b-x}{b-a}e^{ta}+\\frac{x-a}{b-a}e^{tb}$$  \n",
    "\n",
    "setting $\\mathbb{E}\\left[X\\right]=0$:\n",
    "\n",
    "$$\\mathbb{E}\\left[e^{tX}\\right]\\leq\\mathbb{E}\\left[\\frac{b-X}{b-a}e^{ta}+\\frac{X-a}{b-a}e^{tb}\\right]=\\frac{b}{b-a}e^{ta}+\\frac{-a}{b-a}e^{tb}=e^{\\phi(t)}$$  \n",
    "\n",
    "where  \n",
    "\n",
    "$$\\phi(t)=log\\left(\\frac{b}{b-a}e^{ta}+\\frac{-a}{b-a}e^{tb}\\right)$$  \n",
    "\n",
    "$$=ta+log\\left(\\frac{b}{b-a}+\\frac{-a}{b-a}e^{t(b-a)}\\right).$$  \n",
    "\n",
    "Now, for any $t>0$,  \n",
    "\n",
    "$$\\phi'(t)=a-\\frac{a}{\\frac{b}{b-a}e^{-t(b-a)}-\\frac{a}{a-b}}$$  \n",
    "\n",
    "$$\\phi''(t)=\\frac{\\alpha}{\\left[(1-\\alpha)e^{-t(b-a)}+\\alpha\\right]} \\frac{(1-\\alpha) e^{-t(b-a)}}{\\left[(1-\\alpha)e^{-t(b-a)}+\\alpha\\right]}(b-a)^2$$  \n",
    "\n",
    "with \n",
    "\n",
    "$$\\alpha = \\frac{-a}{b-a}$$  \n",
    "\n",
    "$\\phi(0)=\\phi'(0)=0$ and $\\phi''(t)=u(1-u)(b-a)^2$ with $u=\\frac{\\alpha}{\\left[(1-\\alpha)e^{-t(b-a)}+\\alpha\\right]}$. As $u\\in\\left[0,1\\right]$, it follows that $u(1-u)\\leq\\frac{1}{4}$ and $\\phi''(t)\\leq\\frac{(b-a)^2}{4}$.   \n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Finally, via [Taylor's Theorem](http://www.math.toronto.edu/courses/mat237y1/20199/notes/Chapter2/S2.6.html#mjx-eqn-ttlr#the-case-k2) (Lemma 2):  \n",
    "\n",
    "$$\\phi(t)=\\phi(0)+t\\phi'(0)+\\frac{t^2}{2}\\phi''(\\theta)\\leq t^2\\frac{(b-a)^2}{8}$$  \n",
    "\n",
    "from which the proposition follows. $\\quad\\square$\n",
    "\n",
    "### 2) Hoeffding's inequality\n",
    "**Proposition:** Let $X_1,...,X_n$ be independent random variables with $X_i$ taking values in $\\left[a_i,b_i\\right]\\forall i\\in n$. Then for any $\\epsilon>0$ and $S_n=\\sum_{i=1}^nX_i$:  \n",
    "\n",
    "$$\\mathbb{P}\\left[S_n-\\mathbb{E}\\left[S_n\\right]\\geq\\epsilon\\right]\\leq e^\\frac{2\\epsilon^2}{\\sum_{i=1}^n(b_i-a_i)^2}$$  \n",
    "\n",
    "$$\\mathbb{P}\\left[S_n-\\mathbb{E}\\left[S_n\\right]\\leq\\epsilon\\right]\\leq e^\\frac{2\\epsilon^2}{\\sum_{i=1}^n(b_i-a_i)^2}$$  \n",
    "\n",
    "**Proof:** Via Hoeffding's lemma and the Chernoff bounding technique, we obtain  \n",
    "\n",
    "$$\\mathbb{P}\\left[S_n-\\mathbb{E}\\left[S_n\\right]\\geq\\epsilon\\right]\\leq e^{-t\\epsilon}\\mathbb{E}\\left[e^{t(S_n-\\mathbb{E}\\left[S_n\\right])}\\right]$$  \n",
    "\n",
    "$$=e^{-t\\epsilon}\\prod_{i=1}^n\\mathbb{E}\\left[e^{t(X_i-\\mathbb{E}\\left[X_i\\right])}\\right]$$  \n",
    "\n",
    "$$\\leq e^{t\\epsilon}\\prod_{i=1}^ne^{t^2(b_i-a_i)^2/8}$$  \n",
    "\n",
    "$$=e^{-t\\epsilon}e^{t^2\\sum_{i=1}^n(b_i-a_i)^2/8}$$  \n",
    "\n",
    "Finally, with $t=\\frac{4\\epsilon}{\\sum_{i=1}^n(b_i-a_i)^2}$ we get  \n",
    "\n",
    "$$e^{-t\\epsilon}e^{t^2\\sum_{i=1}^n(b_i-a_i)^2/8}$$  \n",
    "\n",
    "$$\\leq e^\\frac{-2\\epsilon^2}{\\sum_{i=1}^n(b_i-a_i)^2}$$  \n",
    "\n",
    "<br>  \n",
    "<br>\n",
    "\n",
    "*Second inequality TBD*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Degeneracy of the Cauchy-Distribution  \n",
    "\n",
    "The pdf of a (standard) Cauchy-Distribution is  \n",
    "\n",
    "$$p(x)=\\frac{1}{\\pi(1+x^2)}$$  \n",
    "\n",
    "Hence, the mean calculates as  \n",
    "\n",
    "$$\\mathbb{E}\\left[X\\right]=\\int_{-\\infty}^{\\infty}x\\frac{1}{\\pi(1+x^2)}dx=\\frac{1}{2\\pi}log(1+x^2)\\Big|^\\infty_{-\\infty}=``\\infty-\\infty``$$  \n",
    "\n",
    "which is an undefined expression and the distribution does not have a mean. Any higher moments are also undefined which shows a limitation of methods using statistical moments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "- Casella, George, and Roger L. Berger. Statistical inference. Vol. 2. Pacific Grove, CA: Duxbury, 2002.\n",
    "- Mittelhammer, Ron C. Mathematical statistics for economics and business. Vol. 78. New York: Springer, 1996.\n",
    "- Mohri, Mehryar, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press, 2018."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
