{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate case: \n",
    "\n",
    "### Mean and variance of a random variable $X$ with density $p(x)$ and support $\\mathcal{X}$:  \n",
    "\n",
    "$$\\mathbb{E}\\left[X\\right]=\\int_{\\mathcal{X}}xp(x)dx\\quad(:=\\mu_X)$$  \n",
    "\n",
    "$$\\mathbb{V}ar\\left[X\\right]=\\int_{\\mathcal{X}}(x-\\mu_X)^2p(x)dx\\quad(:=\\sigma^2_X)$$  \n",
    "\n",
    "and also  \n",
    "\n",
    "$$\\mathbb{V}ar\\left[X\\right]=\\mathbb{E}\\left[X^2\\right]-\\mathbb{E}\\left[X\\right]^2$$  \n",
    "\n",
    "### Empirical estimators given samples $x_1,...,x_n$ drawn i.i.d. from $X$:  \n",
    "\n",
    "$$\\hat{\\mu}_X=\\frac{1}{n}\\sum_{i=1}^nx_i\\quad(:=\\bar{x})$$  \n",
    "\n",
    "$$\\hat{\\sigma}_X^2=\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})^2\\quad(:=s^2)$$ \n",
    "\n",
    "### Properties of the mean  \n",
    "\n",
    "-  $\\mathbb{E}\\left[a\\cdot X\\right]=a\\cdot\\mathbb{E}\\left[X\\right]$\n",
    "-  $\\mathbb{E}\\left[X+Y\\right]=\\mathbb{E}\\left[X\\right]+\\mathbb{E}\\left[Y\\right]$\n",
    "-  $\\mathbb{E}\\left[X^2\\right]=\\mathbb{V}ar\\left[X\\right]+\\mathbb{E}\\left[X\\right]^2$  \n",
    "\n",
    "### Properties of variance  \n",
    "\n",
    "- $\\mathbb{V}ar\\left[a\\cdot X\\right]=a^2\\cdot\\mathbb{V}ar\\left[X\\right]$\n",
    "- $\\mathbb{V}ar\\left[X+Y\\right]=\\mathbb{V}ar\\left[X\\right]+\\mathbb{V}ar\\left[Y\\right]+2Cov(X,Y)$\n",
    "- $\\mathbb{V}ar\\left[X^2\\right]=\\mathbb{E}\\left[X^4\\right]-\\mathbb{E}\\left[X^2\\right]^2=\\mathbb{E}\\left[X^4\\right]-\\mathbb{V}ar\\left[X\\right]^2-2\\mathbb{V}ar\\left[X\\right]\\mathbb{E}\\left[X\\right]^2-\\mathbb{E}\\left[X\\right]^4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical moments  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate case:  \n",
    "\n",
    "\n",
    "- r-th moment of a r.v. $X$:  \n",
    "\n",
    "$$m_r(X)=\\mathbb{E}\\left[X^r\\right]$$  \n",
    "\n",
    "- r-th absolute moment of a r.v. $X$:  \n",
    "\n",
    "$$M_r(X)=\\mathbb{E}\\left[|X|^r\\right]$$  \n",
    "\n",
    "- r-th central moment of a r.v. $X$:  \n",
    "\n",
    "$$\\mu_r(X)=\\mathbb{E}\\left[(X-\\mu_X)^r\\right]$$  \n",
    "\n",
    "- r-th central absolute moment of a r.v. $X$:  \n",
    "\n",
    "$$\\mu_r(X)=\\mathbb{E}\\left[|X-\\mu_X|^r\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate case:  \n",
    "\n",
    "- r-th and s-th joint moment of r.v. $X$ and $Y$:  \n",
    "\n",
    "$$m_{rs}=\\mathbb{E}_{XY}\\left[X^rY^s\\right]$$  \n",
    "\n",
    "- r-th and s-th joint central moment of r.v. $X$ and $Y$:  \n",
    "\n",
    "$$\\mu_{rs}=\\mathbb{E}_{XY}\\left[(X-\\mu_X)^r(Y-\\mu_Y)^s\\right]$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convergence of a series of random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Convergence in distribution:  \n",
    "\n",
    "Let $F_n(x)$ be the cdf corresponding to r.v. $X_n$ at point $x\\in\\mathcal{X}$, then convergence in distribution is defined as:\n",
    "\n",
    "$$\\lim_{n\\rightarrow\\infty}F_n(x)=F(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Convergence in probability:  \n",
    "\n",
    "For all $\\epsilon>0$ we have:\n",
    "\n",
    "$$\\lim_{n\\rightarrow\\infty}\\mathbb{P}(|X_n-X|>\\epsilon)=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Almost sure convergence:  \n",
    "\n",
    "$$\\mathbb{P}(\\lim_{n\\rightarrow\\infty}X_n=X)=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Convergence in mean\n",
    "\n",
    "If $\\mathbb{E}\\left[|X_n|^r\\right]$ and $\\mathbb{E}\\left[|X|^r\\right]$ exist, $X_n$ converges to $X$ in the r-th mean if  \n",
    "\n",
    "$$\\lim_{n\\rightarrow\\infty}\\mathbb{E}\\left[|X_n-X|^r\\right]=0$$  \n",
    "Where the most common case is $r=2$ (\"convergence in mean-square\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important inequalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov's Inequality:  \n",
    "\n",
    "### Core theorem\n",
    "**Proposition:** Let $X$ be a random variable with $X\\geq0$. Then for any positive real number $a$ and if $\\mathbb{E}\\left[X\\right]$ exists:  \n",
    "\n",
    "$$\\mathbb{P}(X\\geq a)\\leq\\frac{\\mathbb{E}\\left[X\\right]}{a}$$  \n",
    "\n",
    "**Proof** By definition, we have $\\mathbb{E}\\left[X\\right]=\\int_{\\mathcal{X}}xp(x)dx$, which we can split as  \n",
    "\n",
    "$$\\mathbb{E}\\left[X\\right]=\\int_{x\\geq a}xp(x)dx+\\int_{x< a}xp(x)dx$$  \n",
    "\n",
    "and since by assumption $x\\geq0$, we have  \n",
    "\n",
    "$$\\geq \\int_{x\\geq a}ap(x)dx=a\\cdot\\int_{x\\geq a}p(x)dx$$  \n",
    "$$=a\\cdot\\mathbb{P}(X\\geq a)\\quad\\quad\\square$$  \n",
    "\n",
    "### Chernoff bounding technique\n",
    "We can use Markov's Inequality to derive for every $t>0$  \n",
    "\n",
    "$$\\mathbb{P}(X\\geq a)\\leq\\mathbb{P}(e^{tX}\\geq e^{ta})\\leq\\frac{\\mathbb{E}\\left[e^{tX}\\right]}{e^{ta}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chebyshev's Inequality  \n",
    "**Proposition** Let $X$ be any random variable with finite mean and variance, then for any positive real $a$, we have:  \n",
    "<br>\n",
    "$$\\mathbb{P}(|X-\\mathbb{E}(X)|\\geq a)\\leq \\frac{\\mathbb{V}ar(X)}{a^2}$$\n",
    "<br>\n",
    "**Proof** Let $Y=(X-\\mathbb{E}\\left[X\\right])^2$ - $Y$ is therefore non-negative with $\\mathbb{E}\\left[Y\\right]=\\mathbb{V}ar(X)$. Then use Markov's inequality to obtain  \n",
    "<br>\n",
    "$$\\mathbb{P}(Y\\geq a^2)\\leq\\frac{\\mathbb{E}\\left[Y\\right]}{a^2}=\\frac{\\mathbb{V}ar(X)}{a^2}$$\n",
    "<br>\n",
    "taking the square root of on the LHS, we get  \n",
    "\n",
    "$$\\mathbb{P}(Y\\geq a^2)=\\mathbb{P}((X-\\mathbb{E}\\left[X\\right])^2\\geq a^2)$$  \n",
    "$$=\\mathbb{P}(|X-\\mathbb{E}\\left[X\\right]|\\geq a)$$  \n",
    "\n",
    "and the result follows from plugging in into the inequality above  $\\quad\\quad\\square$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jensen's inequality   \n",
    "\n",
    "**Proposition** Let $g(\\cdot)$ be a convex function, then for a random variable $X$ if $\\mathbb{E}\\left[X\\right]$ exists:  \n",
    "\n",
    "$$g(\\mathbb{E}\\left[X\\right])\\leq\\mathbb{E}\\left[g(X)\\right]$$  \n",
    "**Proof** With $L(x)=ax+b$ as a line tangent to $g(x)$ at $\\mathbb{E}\\left[X\\right]$, we have  \n",
    "\n",
    "$$\\mathbb{E}\\left[g(X)\\right]\\geq\\mathbb{E}\\left[L(X)\\right]=\\mathbb{E}\\left[(ax+b)\\right]=a\\mathbb{E}\\left[a\\right]+b=L(\\mathbb{E}\\left[X\\right])=g(\\mathbb{E}\\left[X\\right])\\quad\\square$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cauchy-Schwarz inequality\n",
    "\n",
    "**Proposition** For r.v. $X$ and $Y$, we have  \n",
    "\n",
    "$$|\\mathbb{E}\\left[XY\\right]|^2\\leq\\mathbb{E}\\left[X^2\\right]\\mathbb{E}\\left[Y^2\\right]$$  \n",
    "\n",
    "**Proof** Define $W=(X-aY)^2$, then  \n",
    "\n",
    "$$0\\leq \\mathbb{E}\\left[(X-aY)^2\\right]=\\mathbb{E}\\left[X^2-2aXY+a^2Y^2\\right]=$$  \n",
    "$$\\mathbb{E}\\left[X^2\\right]-2a\\mathbb{E}\\left[XY\\right]+a^2\\mathbb{E}\\left[Y^2\\right]\\quad\\forall a\\geq0;$$  \n",
    "\n",
    "setting  \n",
    "\n",
    "$$a=\\frac{\\mathbb{E}\\left[XY\\right]}{\\mathbb{E}\\left[Y^2\\right]}$$  \n",
    "\n",
    "we get  \n",
    "\n",
    "$$0\\leq\\mathbb{E}\\left[X^2\\right]-2a\\mathbb{E}\\left[XY\\right]+a^2\\mathbb{E}\\left[Y^2\\right]=$$    \n",
    "\n",
    "$$\\mathbb{E}\\left[X^2\\right]-2\\frac{\\mathbb{E}\\left[XY\\right]}{\\mathbb{E}\\left[Y^2\\right]}\\mathbb{E}\\left[XY\\right]+\\frac{\\mathbb{E}\\left[XY\\right]^2}{\\mathbb{E}\\left[Y^2\\right]^2}\\mathbb{E}\\left[Y^2\\right]=$$  \n",
    "\n",
    "$$\\mathbb{E}\\left[X^2\\right]-\\frac{\\mathbb{E}\\left[XY\\right]^2}{\\mathbb{E}\\left[Y^2\\right]^2}$$  \n",
    "\n",
    "and the proposition follows$\\quad\\square$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hoeffding's inequality  \n",
    "\n",
    "### 1) Hoeffding's lemma  \n",
    "**Proposition:** For $X$ a random variable with $\\mathbb{E}\\left[X\\right]=0$ and $a\\leq X\\leq b,\\,b>a$ and any $t>0$:  \n",
    "\n",
    "$$\\mathbb{E}\\left[e^{tX}\\right]\\leq e^\\frac{t^2(b-a)^2}{8}$$  \n",
    "**Proof:** By convexity of $e^x$, we have  \n",
    "\n",
    "$$e^{tx}\\leq\\frac{b-x}{b-a}e^{ta}+\\frac{x-a}{b-a}e^{tb}$$  \n",
    "\n",
    "setting $\\mathbb{E}\\left[X\\right]=0$:\n",
    "\n",
    "$$\\mathbb{E}\\left[e^{tX}\\right]\\leq\\mathbb{E}\\left[\\frac{b-X}{b-a}e^{ta}+\\frac{X-a}{b-a}e^{tb}\\right]=\\frac{b}{b-a}e^{ta}+\\frac{-a}{b-a}e^{tb}=e^{\\phi(t)}$$  \n",
    "\n",
    "where  \n",
    "\n",
    "$$\\phi(t)=log\\left(\\frac{b}{b-a}e^{ta}+\\frac{-a}{b-a}e^{tb}\\right)$$  \n",
    "\n",
    "$$=ta+log\\left(\\frac{b}{b-a}+\\frac{-a}{b-a}e^{t(b-a)}\\right).$$  \n",
    "\n",
    "Now, for any $t>0$,  \n",
    "\n",
    "$$\\phi'(t)=a-\\frac{a}{\\frac{b}{b-a}e^{-t(b-a)}-\\frac{a}{a-b}}$$  \n",
    "\n",
    "$$\\phi''(t)=\\frac{\\alpha}{\\left[(1-\\alpha)e^{-t(b-a)}+\\alpha\\right]} \\frac{(1-\\alpha) e^{-t(b-a)}}{\\left[(1-\\alpha)e^{-t(b-a)}+\\alpha\\right]}(b-a)^2$$  \n",
    "\n",
    "with \n",
    "\n",
    "$$\\alpha = \\frac{-a}{b-a}$$  \n",
    "\n",
    "$\\phi(0)=\\phi'(0)=0$ and $\\phi''(t)=u(1-u)(b-a)^2$ with $u=\\frac{\\alpha}{\\left[(1-\\alpha)e^{-t(b-a)}+\\alpha\\right]}$. As $u\\in\\left[0,1\\right]$, it follows that $u(1-u)\\leq\\frac{1}{4}$ and $\\phi''(t)\\leq\\frac{(b-a)^2}{4}$.   \n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Finally, via [Taylor's Theorem](http://www.math.toronto.edu/courses/mat237y1/20199/notes/Chapter2/S2.6.html#mjx-eqn-ttlr#the-case-k2) (Lemma 2):  \n",
    "\n",
    "$$\\phi(t)=\\phi(0)+t\\phi'(0)+\\frac{t^2}{2}\\phi''(\\theta)\\leq t^2\\frac{(b-a)^2}{8}$$  \n",
    "\n",
    "from which the proposition follows. $\\quad\\square$\n",
    "\n",
    "### 2) Hoeffding's inequality\n",
    "**Proposition:** Let $X_1,...,X_n$ be independent random variables with $X_i$ taking values in $\\left[a_i,b_i\\right]\\forall i\\in n$. Then for any $\\epsilon>0$ and $S_n=\\sum_{i=1}^nX_i$:  \n",
    "\n",
    "$$\\mathbb{P}\\left[S_n-\\mathbb{E}\\left[S_n\\right]\\geq\\epsilon\\right]\\leq e^\\frac{2\\epsilon^2}{\\sum_{i=1}^n(b_i-a_i)^2}$$  \n",
    "\n",
    "$$\\mathbb{P}\\left[S_n-\\mathbb{E}\\left[S_n\\right]\\leq\\epsilon\\right]\\leq e^\\frac{2\\epsilon^2}{\\sum_{i=1}^n(b_i-a_i)^2}$$  \n",
    "\n",
    "**Proof:** Via Hoeffding's lemma and the Chernoff bounding technique, we obtain  \n",
    "\n",
    "$$\\mathbb{P}\\left[S_n-\\mathbb{E}\\left[S_n\\right]\\geq\\epsilon\\right]\\leq e^{-t\\epsilon}\\mathbb{E}\\left[e^{t(S_n-\\mathbb{E}\\left[S_n\\right])}\\right]$$  \n",
    "\n",
    "$$=e^{-t\\epsilon}\\prod_{i=1}^n\\mathbb{E}\\left[e^{t(X_i-\\mathbb{E}\\left[X_i\\right])}\\right]$$  \n",
    "\n",
    "$$\\leq e^{t\\epsilon}\\prod_{i=1}^ne^{t^2(b_i-a_i)^2/8}$$  \n",
    "\n",
    "$$=e^{-t\\epsilon}e^{t^2\\sum_{i=1}^n(b_i-a_i)^2/8}$$  \n",
    "\n",
    "Finally, with $t=\\frac{4\\epsilon}{\\sum_{i=1}^n(b_i-a_i)^2}$ we get  \n",
    "\n",
    "$$e^{-t\\epsilon}e^{t^2\\sum_{i=1}^n(b_i-a_i)^2/8}$$  \n",
    "\n",
    "$$\\leq e^\\frac{-2\\epsilon^2}{\\sum_{i=1}^n(b_i-a_i)^2}$$  \n",
    "\n",
    "<br>  \n",
    "<br>\n",
    "\n",
    "*Second inequality TBD*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Degeneracy of the Cauchy-Distribution  \n",
    "\n",
    "The pdf of a (standard) Cauchy-Distribution is  \n",
    "\n",
    "$$p(x)=\\frac{1}{\\pi(1+x^2)}$$  \n",
    "\n",
    "Hence, the mean calculates as  \n",
    "\n",
    "$$\\mathbb{E}\\left[X\\right]=\\int_{-\\infty}^{\\infty}x\\frac{1}{\\pi(1+x^2)}dx=\\frac{1}{2\\pi}log(1+x^2)\\Big|^\\infty_{-\\infty}=``\\infty-\\infty``$$  \n",
    "\n",
    "which is an undefined expression and the distribution does not have a mean. Any higher moments are also undefined which shows a limitation of methods using statistical moments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "- Casella, George, and Roger L. Berger. Statistical inference. Vol. 2. Pacific Grove, CA: Duxbury, 2002.\n",
    "- Mittelhammer, Ron C. Mathematical statistics for economics and business. Vol. 78. New York: Springer, 1996.\n",
    "- Mohri, Mehryar, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press, 2018."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
